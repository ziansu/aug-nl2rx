# handle the raw data generated by `generate_regex_data.py`
from preprocess_data import process_tokens
import random

def extract_from(filename):
    "extract from file"
    desc = []
    regex = []
    with open(filename, 'r') as f:
        for line in f.readlines():
            if line[0] == 'D':
                desc.append(line[6:])
            elif line[0] == 'R' and line[1] == 'e':
                regex.append(line[11:])
    return desc, regex


def remove_overlap(existing_targ, raw_targ, raw_src):
    if existing_targ[-1][-1] != '\n':
        existing_targ[-1] += '\n'
    existing_targ = set(existing_targ)
    new_src = []
    new_targ = []
    for src, targ in zip(raw_src, raw_targ):
        if targ not in existing_targ:
            new_src.append(src)
            new_targ.append(targ)
    return new_src, new_targ


def main():
    generated_raw_filepath = 'generated_raw.txt'
    existing_target_filepath = '../../data/NL-RX-Turk/targ.txt'
    output_path = '../../data/data-synthesized/'

    desc, regex = extract_from(generated_raw_filepath)
    # print([(d,r) for d, r in zip(desc[:5], regex[:5])])
    # print(len(desc), len(regex))
    with open(existing_target_filepath, 'r') as f:
        old_regex = f.readlines()
    new_desc, new_regex = remove_overlap(old_regex, regex, desc)
    # print(len(new_desc), len(new_regex))    # 10382 -> 4793
    with open(output_path + 'src.txt', 'w') as f1, open(output_path + 'targ.txt', 'w') as f2:
        f1.writelines(new_desc)
        f2.writelines(new_regex)
    

def merge():
    "merge raw, just a prototype"
    train_path = '../../data/data-eval-turk/'
    new_desc_lines, new_regex_lines = process_tokens('../../data/data-synthesized/')
    with open(train_path+'src-train.txt', 'r') as f1, open(train_path+'targ-train.txt', 'r') as f2:
        old_desc_lines = f1.readlines()
        old_regex_lines = f2.readlines()
        desc_lines = [desc + '\n' for desc in new_desc_lines] + old_desc_lines
        regex_lines = [regex + '\n' for regex in new_regex_lines] + old_regex_lines
    with open(train_path+'src-train-merged.txt','w') as f1, open(train_path+'targ-train-merged.txt','w') as f2:
        f1.writelines(desc_lines)
        f2.writelines(regex_lines)

def prepare_for_bt():
    "process but not merge"
    # train_path = '../../data/data-eval-turk/'
    synth_path = '../../data/data-synthesized/'
    new_desc_lines, new_regex_lines = process_tokens(synth_path)
    with open(synth_path+'src-anonymized.txt','w') as f1, open(synth_path+'targ-anonymized.txt','w') as f2:
        f1.writelines([desc + '\n' for desc in new_desc_lines])
        f2.writelines([regex + '\n' for regex in new_regex_lines])

def merge_bt():
    train_path = '../../data/data-eval-turk/'
    synth_path = '../../data/data-synthesized/'
    with open('../output/test_0.output', 'r') as f:
        new_desc_lines = f.readlines()
    with open(synth_path + 'targ-anonymized.txt', 'r') as f:
        new_regex_lines = f.readlines()
    assert(new_regex_lines[-1] != '\n')
    print(new_desc_lines[0])

    def __sample_desc(desc_line, beam_size=5):
        "Temporary processing of multiple descriptions in a single line"
        desc_list = desc_line.strip().split(' | ')
        assert(len(desc_list) == beam_size)    # current beam size

        # MAP (not very interesting though 233)
        # return desc_list[0]
        
        # restricted sampling
        return desc_list[random.randint(0, beam_size-1)] + '\n'


    with open(train_path+'src-train.txt', 'r') as f1, open(train_path+'targ-train.txt', 'r') as f2:
        old_desc_lines = f1.readlines()
        old_regex_lines = f2.readlines()
        desc_lines = [__sample_desc(desc) for desc in new_desc_lines] + old_desc_lines
        regex_lines = new_regex_lines + old_regex_lines
    with open(train_path+'src-train-merged.txt','w') as f1, open(train_path+'targ-train-merged.txt','w') as f2:
        f1.writelines(desc_lines)
        f2.writelines(regex_lines)

    
if __name__ == '__main__':
    # main()
    # merge()
    # prepare_for_bt()
    merge_bt()